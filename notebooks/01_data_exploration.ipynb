{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MITSUI Commodity Prediction Challenge - Data Exploration\n",
    "\n",
    "This notebook provides a comprehensive exploratory data analysis (EDA) of the competition data.\n",
    "\n",
    "## Contents\n",
    "1. Data Loading and Basic Information\n",
    "2. Missing Data Analysis\n",
    "3. Target Variable Analysis\n",
    "4. Feature Distribution Analysis\n",
    "5. Time Series Patterns\n",
    "6. Cross-Market Correlations\n",
    "7. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path for imports\n",
    "sys.path.append('../src')\n",
    "from utils.data_loader import DataLoader, load_competition_data\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Basic Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all competition data\n",
    "loader, data = load_competition_data('../')\n",
    "\n",
    "# Extract dataframes for easier access\n",
    "train_df = data['train']\n",
    "train_labels_df = data['train_labels']\n",
    "test_df = data['test']\n",
    "target_pairs_df = data['target_pairs']\n",
    "lagged_test_labels = data['lagged_test_labels']\n",
    "\n",
    "print(\"Data loaded successfully!\")\n",
    "print(f\"Training data: {train_df.shape}\")\n",
    "print(f\"Training labels: {train_labels_df.shape}\")\n",
    "print(f\"Test data: {test_df.shape}\")\n",
    "print(f\"Target pairs: {target_pairs_df.shape}\")\n",
    "print(f\"Lagged test labels: {len(lagged_test_labels)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get detailed data information\n",
    "data_info = loader.get_data_info()\n",
    "\n",
    "for name, info in data_info.items():\n",
    "    print(f\"\\n=== {name.upper()} ===\")\n",
    "    print(f\"Shape: {info['shape']}\")\n",
    "    print(f\"Memory usage: {info['memory_usage_mb']:.2f} MB\")\n",
    "    print(f\"Data types: {info['dtypes']}\")\n",
    "    print(f\"Missing values: {info['missing_values']} ({info['missing_percentage']:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature categories analysis\n",
    "feature_categories = loader.get_feature_categories()\n",
    "\n",
    "print(\"Feature Categories:\")\n",
    "total_features = 0\n",
    "for category, features in feature_categories.items():\n",
    "    print(f\"{category.upper()}: {len(features)} features\")\n",
    "    total_features += len(features)\n",
    "    \n",
    "print(f\"\\nTotal features: {total_features}\")\n",
    "\n",
    "# Visualize feature distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "categories = list(feature_categories.keys())\n",
    "counts = [len(features) for features in feature_categories.values()]\n",
    "\n",
    "bars = ax.bar(categories, counts, alpha=0.7)\n",
    "ax.set_title('Number of Features by Category', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Number of Features')\n",
    "ax.set_xlabel('Feature Category')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, count in zip(bars, counts):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 5,\n",
    "            f'{count}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Missing Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get missing data summaries\n",
    "missing_summaries = loader.get_missing_data_summary()\n",
    "\n",
    "# Analyze missing data in training features\n",
    "print(\"=== TRAINING FEATURES MISSING DATA ===\")\n",
    "train_missing = missing_summaries['train']\n",
    "features_with_missing = train_missing[train_missing > 0].sort_values(ascending=False)\n",
    "\n",
    "if len(features_with_missing) > 0:\n",
    "    print(f\"Features with missing values: {len(features_with_missing)}\")\n",
    "    print(\"\\nTop 10 features with most missing values:\")\n",
    "    print(features_with_missing.head(10))\n",
    "    \n",
    "    # Visualize missing data patterns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Missing values by feature category\n",
    "    missing_by_category = {}\n",
    "    for category, features in feature_categories.items():\n",
    "        category_missing = train_missing[features].sum()\n",
    "        missing_by_category[category] = category_missing\n",
    "    \n",
    "    ax1.bar(missing_by_category.keys(), missing_by_category.values(), alpha=0.7)\n",
    "    ax1.set_title('Missing Values by Feature Category')\n",
    "    ax1.set_ylabel('Number of Missing Values')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Top features with missing values\n",
    "    if len(features_with_missing) >= 10:\n",
    "        top_missing = features_with_missing.head(10)\n",
    "    else:\n",
    "        top_missing = features_with_missing\n",
    "    \n",
    "    ax2.barh(range(len(top_missing)), top_missing.values, alpha=0.7)\n",
    "    ax2.set_yticks(range(len(top_missing)))\n",
    "    ax2.set_yticklabels([name.replace('_', '\\n') if len(name) > 20 else name for name in top_missing.index])\n",
    "    ax2.set_title('Top Features with Missing Values')\n",
    "    ax2.set_xlabel('Number of Missing Values')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\nelse:\n",
    "    print(\"No missing values in training features!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze missing data in training labels\n",
    "print(\"\\n=== TRAINING LABELS MISSING DATA ===\")\n",
    "labels_missing = missing_summaries['train_labels']\n",
    "targets_with_missing = labels_missing[labels_missing > 0].sort_values(ascending=False)\n",
    "\n",
    "print(f\"Targets with missing values: {len(targets_with_missing)}\")\n",
    "print(f\"Total missing label values: {targets_with_missing.sum()}\")\n",
    "\n",
    "if len(targets_with_missing) > 0:\n",
    "    # Missing data statistics\n",
    "    print(f\"\\nMissing data statistics:\")\n",
    "    print(f\"Min missing per target: {targets_with_missing.min()}\")\n",
    "    print(f\"Max missing per target: {targets_with_missing.max()}\")\n",
    "    print(f\"Mean missing per target: {targets_with_missing.mean():.1f}\")\n",
    "    print(f\"Median missing per target: {targets_with_missing.median():.1f}\")\n",
    "    \n",
    "    # Visualize missing patterns in targets\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Histogram of missing values\n",
    "    ax1.hist(targets_with_missing.values, bins=20, alpha=0.7, edgecolor='black')\n",
    "    ax1.set_title('Distribution of Missing Values per Target')\n",
    "    ax1.set_xlabel('Number of Missing Values')\n",
    "    ax1.set_ylabel('Number of Targets')\n",
    "    \n",
    "    # Missing percentage over time (if there's a time pattern)\n",
    "    missing_by_date = train_labels_df.groupby('date_id').apply(\n",
    "        lambda x: x.drop('date_id', axis=1).isnull().sum().sum()\n",
    "    )\n",
    "    \n",
    "    ax2.plot(missing_by_date.index, missing_by_date.values, alpha=0.7)\n",
    "    ax2.set_title('Missing Values Over Time')\n",
    "    ax2.set_xlabel('Date ID')\n",
    "    ax2.set_ylabel('Total Missing Values')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Target Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze target pairs\n",
    "print(\"=== TARGET PAIRS ANALYSIS ===\")\n",
    "print(f\"Total targets: {len(target_pairs_df)}\")\n",
    "print(f\"Unique lags: {sorted(target_pairs_df['lag'].unique())}\")\n",
    "print(f\"Lag distribution:\")\n",
    "print(target_pairs_df['lag'].value_counts().sort_index())\n",
    "\n",
    "# Analyze target types (single asset vs spreads)\n",
    "target_pairs_df['is_spread'] = target_pairs_df['pair'].str.contains(' - ')\n",
    "spread_count = target_pairs_df['is_spread'].sum()\n",
    "single_count = len(target_pairs_df) - spread_count\n",
    "\n",
    "print(f\"\\nTarget types:\")\n",
    "print(f\"Single assets: {single_count}\")\n",
    "print(f\"Spreads: {spread_count}\")\n",
    "\n",
    "# Show example targets\n",
    "print(f\"\\nExample single asset targets:\")\n",
    "single_targets = target_pairs_df[~target_pairs_df['is_spread']]['pair'].head(5)\n",
    "for i, target in enumerate(single_targets, 1):\n",
    "    print(f\"{i}. {target}\")\n",
    "\n",
    "print(f\"\\nExample spread targets:\")\n",
    "spread_targets = target_pairs_df[target_pairs_df['is_spread']]['pair'].head(5)\n",
    "for i, target in enumerate(spread_targets, 1):\n",
    "    print(f\"{i}. {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze target value distributions\n",
    "target_columns = [col for col in train_labels_df.columns if col.startswith('target_')]\n",
    "sample_targets = target_columns[:12]  # Analyze first 12 targets\n",
    "\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "target_stats = []\n",
    "\n",
    "for i, target in enumerate(sample_targets):\n",
    "    # Get non-null values\n",
    "    values = train_labels_df[target].dropna()\n",
    "    \n",
    "    if len(values) > 0:\n",
    "        # Plot distribution\n",
    "        axes[i].hist(values, bins=30, alpha=0.7, edgecolor='black')\n",
    "        axes[i].set_title(f'{target}\\n(n={len(values)})', fontsize=10)\n",
    "        axes[i].set_xlabel('Value')\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "        \n",
    "        # Calculate statistics\n",
    "        stats = {\n",
    "            'target': target,\n",
    "            'count': len(values),\n",
    "            'mean': values.mean(),\n",
    "            'std': values.std(),\n",
    "            'min': values.min(),\n",
    "            'max': values.max(),\n",
    "            'skew': values.skew(),\n",
    "            'kurtosis': values.kurtosis()\n",
    "        }\n",
    "        target_stats.append(stats)\n",
    "\n",
    "plt.suptitle('Target Value Distributions (First 12 Targets)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display statistics table\n",
    "stats_df = pd.DataFrame(target_stats)\n",
    "print(\"\\nTarget Statistics Summary:\")\n",
    "print(stats_df.round(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze target correlations\n",
    "print(\"=== TARGET CORRELATIONS ===\")\n",
    "\n",
    "# Calculate correlation matrix for a subset of targets (to avoid memory issues)\n",
    "sample_targets_for_corr = target_columns[:50]  # First 50 targets\n",
    "target_corr_matrix = train_labels_df[sample_targets_for_corr].corr()\n",
    "\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "mask = np.triu(np.ones_like(target_corr_matrix, dtype=bool))\n",
    "sns.heatmap(target_corr_matrix, mask=mask, cmap='RdBu_r', center=0,\n",
    "            square=True, annot=False, fmt='.2f', cbar_kws={'shrink': 0.8})\n",
    "plt.title('Target Correlation Matrix (First 50 Targets)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find highly correlated target pairs\n",
    "high_corr_pairs = []\n",
    "for i in range(len(sample_targets_for_corr)):\n",
    "    for j in range(i+1, len(sample_targets_for_corr)):\n",
    "        corr = target_corr_matrix.iloc[i, j]\n",
    "        if abs(corr) > 0.8 and not pd.isna(corr):\n",
    "            high_corr_pairs.append({\n",
    "                'target1': sample_targets_for_corr[i],\n",
    "                'target2': sample_targets_for_corr[j],\n",
    "                'correlation': corr\n",
    "            })\n",
    "\n",
    "if high_corr_pairs:\n",
    "    print(f\"\\nHighly correlated target pairs (|corr| > 0.8): {len(high_corr_pairs)}\")\n",
    "    high_corr_df = pd.DataFrame(high_corr_pairs)\n",
    "    print(high_corr_df.head(10))\nelse:\n",
    "    print(\"\\nNo highly correlated target pairs found (|corr| > 0.8)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature distributions by category\n",
    "print(\"=== FEATURE DISTRIBUTION ANALYSIS ===\")\n",
    "\n",
    "# Sample features from each category for analysis\n",
    "sample_features = {}\n",
    "for category, features in feature_categories.items():\n",
    "    # Sample up to 3 features from each category\n",
    "    sample_size = min(3, len(features))\n",
    "    sample_features[category] = features[:sample_size]\n",
    "    \n",
    "# Create subplots for feature distributions\n",
    "fig, axes = plt.subplots(4, 3, figsize=(15, 16))\n",
    "\n",
    "for cat_idx, (category, features) in enumerate(sample_features.items()):\n",
    "    for feat_idx, feature in enumerate(features):\n",
    "        if feat_idx < 3:  # Only plot up to 3 features per category\n",
    "            values = train_df[feature].dropna()\n",
    "            \n",
    "            if len(values) > 0:\n",
    "                axes[cat_idx, feat_idx].hist(values, bins=50, alpha=0.7, edgecolor='black')\n",
    "                axes[cat_idx, feat_idx].set_title(f'{category.upper()}\\n{feature}', fontsize=10)\n",
    "                axes[cat_idx, feat_idx].set_xlabel('Value')\n",
    "                axes[cat_idx, feat_idx].set_ylabel('Frequency')\n",
    "                \n",
    "                # Add basic statistics as text\n",
    "                mean_val = values.mean()\n",
    "                std_val = values.std()\n",
    "                axes[cat_idx, feat_idx].axvline(mean_val, color='red', linestyle='--', alpha=0.7, label=f'Mean: {mean_val:.2f}')\n",
    "                axes[cat_idx, feat_idx].legend(fontsize=8)\n",
    "\n",
    "plt.suptitle('Feature Distributions by Category (Sample Features)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary by feature category\n",
    "print(\"\\n=== FEATURE STATISTICS BY CATEGORY ===\")\n",
    "\n",
    "category_stats = {}\n",
    "for category, features in feature_categories.items():\n",
    "    category_data = train_df[features]\n",
    "    \n",
    "    stats = {\n",
    "        'feature_count': len(features),\n",
    "        'mean_range': (category_data.mean().min(), category_data.mean().max()),\n",
    "        'std_range': (category_data.std().min(), category_data.std().max()),\n",
    "        'missing_values': category_data.isnull().sum().sum(),\n",
    "        'missing_percentage': (category_data.isnull().sum().sum() / category_data.size) * 100\n",
    "    }\n",
    "    category_stats[category] = stats\n",
    "    \n",
    "    print(f\"\\n{category.upper()}:\")\n",
    "    print(f\"  Features: {stats['feature_count']}\")\n",
    "    print(f\"  Mean range: {stats['mean_range'][0]:.2f} to {stats['mean_range'][1]:.2f}\")\n",
    "    print(f\"  Std range: {stats['std_range'][0]:.2f} to {stats['std_range'][1]:.2f}\")\n",
    "    print(f\"  Missing: {stats['missing_values']} ({stats['missing_percentage']:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Time Series Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze time series patterns\n",
    "print(\"=== TIME SERIES PATTERNS ===\")\n",
    "\n",
    "# Sample time series from each category\n",
    "sample_ts_features = {}\n",
    "for category, features in feature_categories.items():\n",
    "    # Pick the first feature that has no missing values\n",
    "    for feature in features:\n",
    "        if train_df[feature].isnull().sum() == 0:\n",
    "            sample_ts_features[category] = feature\n",
    "            break\n",
    "    if category not in sample_ts_features and features:  # If all have missing values, pick first\n",
    "        sample_ts_features[category] = features[0]\n",
    "\n",
    "# Plot time series\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (category, feature) in enumerate(sample_ts_features.items()):\n",
    "    if idx < 4:  # Only plot 4 categories\n",
    "        ts_data = train_df[['date_id', feature]].dropna()\n",
    "        \n",
    "        axes[idx].plot(ts_data['date_id'], ts_data[feature], alpha=0.7)\n",
    "        axes[idx].set_title(f'{category.upper()}\\n{feature}', fontsize=12)\n",
    "        axes[idx].set_xlabel('Date ID')\n",
    "        axes[idx].set_ylabel('Value')\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Time Series Patterns by Category', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze target time series patterns\n",
    "sample_target_features = target_columns[:4]  # First 4 targets\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, target in enumerate(sample_target_features):\n",
    "    target_data = train_labels_df[['date_id', target]].dropna()\n",
    "    \n",
    "    axes[idx].plot(target_data['date_id'], target_data[target], alpha=0.7, color='red')\n",
    "    axes[idx].set_title(f'{target}', fontsize=12)\n",
    "    axes[idx].set_xlabel('Date ID')\n",
    "    axes[idx].set_ylabel('Target Value')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    axes[idx].axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "\n",
    "plt.suptitle('Target Time Series Patterns', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive data quality assessment\n",
    "print(\"=== DATA QUALITY ASSESSMENT ===\")\n",
    "\n",
    "# Data validation results\n",
    "validation_results = loader.validate_data_consistency()\n",
    "print(\"Data Consistency Checks:\")\n",
    "for check, result in validation_results.items():\n",
    "    status = \"✓ PASS\" if result else \"✗ FAIL\"\n",
    "    print(f\"  {check}: {status}\")\n",
    "\n",
    "# Check for duplicate rows\n",
    "print(\"\\nDuplicate Row Analysis:\")\n",
    "train_duplicates = train_df.duplicated().sum()\n",
    "test_duplicates = test_df.duplicated().sum() \n",
    "labels_duplicates = train_labels_df.duplicated().sum()\n",
    "print(f\"  Training data duplicates: {train_duplicates}\")\n",
    "print(f\"  Test data duplicates: {test_duplicates}\")\n",
    "print(f\"  Training labels duplicates: {labels_duplicates}\")\n",
    "\n",
    "# Check for infinite or extremely large values\n",
    "print(\"\\nExtreme Values Analysis:\")\n",
    "numeric_cols = train_df.select_dtypes(include=[np.number]).columns\n",
    "inf_count = np.isinf(train_df[numeric_cols]).sum().sum()\n",
    "very_large_count = (np.abs(train_df[numeric_cols]) > 1e10).sum().sum()\n",
    "print(f\"  Infinite values: {inf_count}\")\n",
    "print(f\"  Very large values (>1e10): {very_large_count}\")\n",
    "\n",
    "# Check date_id consistency\n",
    "print(\"\\nDate ID Analysis:\")\n",
    "train_date_range = (train_df['date_id'].min(), train_df['date_id'].max())\n",
    "test_date_range = (test_df['date_id'].min(), test_df['date_id'].max())\n",
    "print(f\"  Training date range: {train_date_range}\")\n",
    "print(f\"  Test date range: {test_date_range}\")\n",
    "print(f\"  Training data continuity: {len(train_df)} rows for {train_date_range[1] - train_date_range[0] + 1} date range\")\n",
    "\n",
    "# Data type consistency\n",
    "print(f\"\\nData Types:\")\n",
    "print(f\"  Training data types: {train_df.dtypes.value_counts().to_dict()}\")\n",
    "print(f\"  Test data types: {test_df.dtypes.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== DATA EXPLORATION SUMMARY ===\")\n",
    "print()\n",
    "print(\"📊 DATASET OVERVIEW:\")\n",
    "print(f\"   • Training samples: {len(train_df):,}\")\n",
    "print(f\"   • Test samples: {len(test_df):,}\")\n",
    "print(f\"   • Features: {len(train_df.columns)-1:,}\")\n",
    "print(f\"   • Targets: {len(target_columns):,}\")\n",
    "print()\n",
    "print(\"🎯 TARGET CHARACTERISTICS:\")\n",
    "print(f\"   • Single asset targets: {single_count}\")\n",
    "print(f\"   • Spread targets: {spread_count}\")\n",
    "print(f\"   • Targets with missing data: {len(targets_with_missing)}\")\n",
    "print()\n",
    "print(\"🔍 FEATURE BREAKDOWN:\")\n",
    "for category, features in feature_categories.items():\n",
    "    print(f\"   • {category.upper()}: {len(features)} features\")\n",
    "print()\n",
    "print(\"⚠️  DATA QUALITY ISSUES:\")\n",
    "print(f\"   • Features with missing values: {len(features_with_missing)}\")\n",
    "print(f\"   • Train/test feature mismatch: {'Yes' if not validation_results.get('train_test_features_match', True) else 'No'}\")\n",
    "print(f\"   • Duplicate rows: {train_duplicates + test_duplicates + labels_duplicates}\")\n",
    "print()\n",
    "print(\"🚀 KEY INSIGHTS:\")\n",
    "print(\"   • Data spans time series with sequential date_ids\")\n",
    "print(\"   • Mixed prediction task: single assets + spreads\")\n",
    "print(\"   • Significant missing data patterns, especially in JPX features\")\n",
    "print(\"   • Target values are normalized returns (roughly -0.12 to +0.11)\")\n",
    "print(\"   • US Stock features dominate the feature space (475/557)\")\n",
    "print()\n",
    "print(\"📋 NEXT STEPS:\")\n",
    "print(\"   1. Implement robust missing data handling strategy\")\n",
    "print(\"   2. Create separate models for single assets vs spreads\")\n",
    "print(\"   3. Focus on time series feature engineering\")\n",
    "print(\"   4. Consider feature selection due to high dimensionality\")\n",
    "print(\"   5. Implement proper time series cross-validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save key findings to file\n",
    "findings = {\n",
    "    'dataset_info': data_info,\n",
    "    'feature_categories': {k: len(v) for k, v in feature_categories.items()},\n",
    "    'missing_data_summary': {\n",
    "        'features_with_missing': len(features_with_missing),\n",
    "        'targets_with_missing': len(targets_with_missing)\n",
    "    },\n",
    "    'target_analysis': {\n",
    "        'single_asset_targets': single_count,\n",
    "        'spread_targets': spread_count,\n",
    "        'total_targets': len(target_columns)\n",
    "    },\n",
    "    'data_quality': validation_results\n",
    "}\n",
    "\n",
    "# This would save to a JSON file in a real implementation\n",
    "print(\"Key findings documented for further analysis.\")\n",
    "print(\"\\n📝 EDA Complete! Ready for preprocessing and feature engineering.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}